# AINL-Eval 2025 Shared Task at AINL 2025
Welcome to the repository for the AINL-Eval 2025 shared task on detection of AI-generated scientific abstracts in Russian held as part of the [AINL 2025](https://ainlconf.ru/) conference.

## ‼️ Test data are open!
Predict test set with the model you developed.

In test mode you should classify texts by 5 classes: `['gpt-4-turbo', 'llama-3.3-70b', 'gemma-2-27b', 'human', 'unknown']`, where four classes present in train set and `'unknown'` is a label for the texts generated by the unseen model.

Note, that for test set we used the different unseen model to generate texts for `'unknown'` class.

Make a submission. Ensure, that submission file has 2 columns: `'id'` and `'label'`. The file should be named `'submission.csv'` Submit the result file on [Codalab](https://codalab.lisn.upsaclay.fr/competitions/21895).

Note, that Codalab requires submit file to be archived in zip. You should archive only the submission file, not the directory.

❗️ In test phase only 5 attemts to submit results are available.


## !Train data are open!
Use train set to develop the model!

Predict dev set with the model. Note, that in dev mode you should classify texts by 5 classes: `['gpt-4-turbo', 'llama-3.3-70b', 'gemma-2-27b', 'human', 'unknown']`, where four classes present in train set and `'unknown'` is a label for the texts generated by the unseen model.

Make a submission. Ensure, that submission file has 2 columns: `'id'` and `'label'`. The file should be named `'submission.csv'` Submit the result file on [Codalab](https://codalab.lisn.upsaclay.fr/competitions/21895).

Note, that Codalab requires submit file to be archived in zip. You should archive only the submission file, not the directory.

_After your results will be processed successfully, please, click on "Make your submission public" to publish the results._

## Motivation
In recent years, developing large language models (LLMs) has revolutionized NLP. Nowadays, they are used almost everywhere, and this is undoubtedly a great progress. The generated texts are really close to human-level writing, making it increasingly challenging to differentiate between AI and human-generated content. However, there are some domains where LLMs usage is not desirable such as fake news generation or writing scientific papers. The latter problem is the main focus of this challenge. We aim to detect AI-generated abstracts for the scientific papers in the Russian language to facilitate research and development tools for this language.

## Useful links
Telegram chat: [t.me/AINL_Eval_2025](http://t.me/AINL_Eval_2025)

Codalab: [AINL-Eval 2025](https://codalab.lisn.upsaclay.fr/competitions/21895)

## Task description
Define whether the text is human-written or AI-generated; if it is AI-generated, identify texts generated by a model that is not included in the training data.

## Dataset
The generated texts were obtained using GPT4-Turbo, Gemma2-27B, Llama3.3-70B and two other models. The models were prompted with the title and keywords from the paper. Additionally, we performed some post-processing by removing empty outputs and model-specific text beginnings, while preserving the main content of the abstracts as is.

The training set consists of ~35 000 texts from 10 different domains (around 4 000 texts per domain), where each text is labeled as one of the following: 'human', 'llama-3.3-70b', 'gemma-2-27b', 'gpt-4-turbo'.

The public test set includes ~11 000 texts from the same 10 domains with the same labels, and additional ~2 000 texts generated by an unseen model (unknown to the participants).

The private test set contains ~6 000 texts from 10 different domains (around 600 texts per domain). Among these, only 8 domains overlap with the training set, while 2 domains are not present in the training data. This set includes texts written by humans, generated by the aforementioned models, and by another model unknown to the participants. 

Thus, we invite participants to propose solutions to the following key challenges:

1) Handling data that extends beyond the training set (generalization to new domains).

2) Detecting texts generated by a model not included in the training data (generalization to new models).

## Submission format
The file [submission.csv](submission.csv) provides the submission format.

## Baselines
We provide two baselines:

- TF-IDF + LogReg

- fine-tuning BERT


## Evaluation
Accuracy will be used to evaluate the solutions.

## Participation rules
Final results will be obtained on the private test set and announced during the [AINL 2025](https://ainlconf.ru/) conference. Each team will have 3 attempts to submit final results. After the competition is over, we will publish the ground truths for the test set, so the participants could perform some ablation studies.

The competition does not involve the use of external data, so the participants are encouraged to use only the given training set.

## Important dates
**3 March 2025** – Training and public test sets are released. 

**5 March 2025** – Submissions to the dev phase are open.

**25 March 2025** – Submissions to the private test phase are open. Submissions to the dev phase remain open for further experimentation.

**1 April 2025** – The shared task is closed. Submissions to both dev and private test phases are no longer accepted.

**18-19 April 2025** – AINL 2025 conference. The final results are announced.

**5 May 2025** – Paper submission deadline.



## Organizers
Tatiana Batura (IIS SB RAS)

Elena Bruches (IIS SB RAS, NSU)

Milana Shvenk (NSU)

Valentin Malykh (MIPT University, ITMO University)

